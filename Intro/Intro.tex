\documentclass[../main.tex]{subfiles}

\begin{document}

In 2004, the American Defense Advanced Research Projects Agency(DARPA) held a challenge for autonomous cars on 150-mile obstacle course through the Mojave desert. No one collected the million dollar prize attached to competition, as the top contender managed 7 miles~\citep{darpa2004}. In 2018, Waymo released its safety report announcing its cars have self-driven for over five million autonomous miles in more than twenty five cities~\citep{waymo2018safety}.  

This leap in machine performance for complex tasks has been a theme of the last two decades, largely powered by Deep Learning (DL). Computer vision models have achieved human performance in image classification~\citep{he2015delving}. Deepmind's AlphaGo beat the dominating world champion in go~\citep{silver2017mastering}, a game long seen as out of reach for computers. DL models beat radiologists in screening for lung cancer\citep{ardila2019end}, beat humans in recognising German traffic signs~\citep{cirecsan2012multi}, and achieved professional human level performance in playing Atari 2600 video games~\citep{mnih2015human}, to cite a few examples.

Such successes have demonstrated the potential of deploying ML to handle real-world complex tasks. In this work, we aim to use DL to create virtual sensors~\citep{li2011review} for vehicle dynamics~\citep{schramm2014vehicle}. Virtual sensors create a lot of value in the automotive industry, they "provide increased functionality, safety, and reliability."~\citep{li2011review}. As discussed in ~\citep{graber2018hybrid} modelling vehicle dynamics has classically relied on over-simplified estimators of the underlying dynamics like Kalman filters. DL models are capable of modelling arbitrarily complex functions~\citep{goodfellow2016deep}, which makes them an appealing choice for richer modelling of vehicle dynamics\citep{graber2018hybrid}. We model virtual sensing tasks as a sequence-to-sequence problem~\citep{goodfellow2016deep}, and apply recurrent neural networks(RNN) as a solution.

However a key challenge in deploying DL systems is to estimate the uncertainty in their predictions. There are numerous factors which can cause a model's predictions to be unreliable. We want our models to signal when they are not confident about their predictions. If such signal is available, it makes deployment more feasible. For instance an automated medical diagnosis system can refer uncertain cases to a doctor, and otherwise give a diagnosis. An autonomous driving system may alert the driver or take precautionary measures when its uncertainty is high. A virtual sensor may signal downstream controllers relying on its predictions that uncertainty is high so that appropriate action is taken. 

There are multiple sources for predictive uncertainty in deep learning models~\citep{kendall2017uncertainties}. Our goal in this paper is to understand those sources and design a model which accounts for all the significant sources of uncertainty. A fundamental source of uncertainty is "data which differ significantly from the data used to train the model"~\citep{bishop1994novelty}. Such inputs are referred to as out-of-distribution(OOD) inputs. An example of OOD inputs is driving on an icy road, for an agent trained to drive on dry roads. ML models struggle to give good predictions on OOD inputs, and to exacerbate the problem they can make those mistaken predictions with a high degree of confidence. 

It is common practice today to use ensembling or a Bayesian approach to account for uncertainty from OOD samples~\citep{kendall2017uncertainties, zhu2017deep, lakshminarayanan2017simple}. We argue that while this approach may empirically work in some cases, it is not reliable in general. We show in \cref{sec:epistemic} a simple one-dimensional example where a full Bayesian approach fails to account for the uncertainty of OOD inputs. Instead, properly accounting for OOD uncertainty, a model needs to be aware of how similar an input is to what it has seen during training.

\subsubsection{Contributions}

In this thesis we present a breakdown of the significant sources of uncertainty in machine learning, and how they can be modeled. Based on our analysis we present Compression Recurrent Neural Networks (C-RNN), which can capture complex vehicle dynamics and effectively model predictive uncertainty. We validate our approach on two real world tasks, first modeling vehicle driving trajectory, and the second modelling Unmanned Aerial Vehicle (UAV) flight behavior. For each task we exclude a certain partition of the data to be out-of-distribution, and preform an in-depth analysis of model uncertainty for both in/out-of-distribution inputs. We introduce discrimination and correlation based metrics to evaluate the quality of a model's uncertainty estimates. Moreover we preform extensive comparative studies between C-RNN and MC dropout~\citep{gal2016dropout} in terms of behavior and quality of uncertainty estimates. Our results show that when OOD samples are a significant source of uncertainty, C-RNN significantly out-preforms MC dropout.  




\end{document}

