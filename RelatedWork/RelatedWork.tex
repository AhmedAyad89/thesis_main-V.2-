\documentclass[../main.tex]{subfiles}


\begin{document}

In this section we will explore the methods used in to estimate uncertainty and deal with OOD inputs in deep learning. The approaches discussed in this section can be broken down by the source of uncertainty they focus on. We have methods for estimating parameter uncertainty, and methods for estimating input uncertainty. 

An example of a method focusing on parameter uncertainty is the Bayesian approach discussed in  \ref{sub:bayesian_framework}. The Bayesian approach is related to a wider range of ensembling approaches. Those methods typically assume i.i.d data and focus on estimating the general epistemic uncertainty. 


Similarity based methods are the complement, they operate to detect OOD inputs, which violate the IID assumption.  

\section{Parameter uncertainty}

Ensembling is the idea of using multiple models to perform learning and prediction. We have already discussed the Bayesian approach to learning in \cref{sub:bayesian_framework}, which ensembles over a space of models. There are simpler methods to ensembling, the basic approach being to train multiple models separately and then average their predictions.

For a simple ensemble, let $f_i(x)$ be the prediction of the ith member on $x$, the final prediction is 
$$f(x) = \frac{1}{n} \sum_{i=0}^n f_i(x)$$ 
The uncertainty of the ensembles is expressed in the \emph{disagreement} between the individual members. For regression problems it is the variance of the predictions around the mean $f(x)$. 
$$Var(f_0(x) \cdots f_n(x))$$
We have also discussed the Bayesian approach which is richer than simple ensembles; it accounts for priors, allows for infinite continuous ensembles and complex predictive distributions. However, we can look at $${f_0(x) \cdots f_n(x)}$$ as samples representing the posterior distribution (\textbf{eq.\ref{eq:bayesian_predictive}}).
There are also more complex versions of ensembling. However, a detailed exploration of ensembling is not in our scope. 

The Bayesian approach is principled and works well, when its assumptions are not violated. OOD inputs violates the i.i.d assumption. \citet{ovadia2019can} test multiple ensembling techniques over deep networks, to assess the quality of uncertainty estimates under dataset shift. The general conclusion is that uncertainty estimates' quality degrades under dataset shift. \citet{Mundt2019open} investigated a number of approaches including a variational approximation to Bayesian neural networks. They found that variational approximations to the Bayesian approach underestimate uncertainty for OOD inputs and concluded that while ensembling/Bayesian approaches help, the use of generative modelling may be necessary to obtain reliable uncertainty estimates.

A challenge with ensembling approaches is diversity of the predictions of individual members outside the data distribution. \citet{jain2019maximizing} explicitly maximize the OOD diversity of their ensembles by forcing diverse predictions on randomly sampled inputs(i.e. noise). The motivation being that ensembles can suffer from lack of diversity in predictions, particularly on OOD data. This work is an example of an ensembling approach, which recognises that ensembling is not enough for dealing with OOD data, and adds additional inductive bias to assign high uncertainty to OOD data. 
In the next section, we will look more at methods which explicitly give high uncertainty to OOD data. 


\section{Input uncertainty}
The methods we discuss in this section share a common goal: increase uncertainty for inputs which are different from the training data. 

\subsection{Similarity based}


There is a broad range of OOD detection approaches roughly based on similarity with known in-distribution data. \citep{hautamaki2004outlier, NIPS2009_3723, zhang2009new} use a nearest neighbor graph to determine an outlier score for points. 
\cite{scholkopf2000support} use a one-class support vector classifier to separate the nominal data from everything else. The one-class classification approach is motivated by the idea that the problem being solved is \emph{easier} than density estimation, and gives the information needed to detect OOD inputs.  
\citet{tax2004support} extend this idea to fit a spherical boundary around the data rather than a hyperplane. \cite{erfani2016high, ruff2018deep} extend the one-class approach by adding deep networks as features extractors for the support-vector classifier. The goal of those approaches is to provide a classifier which can be used to accept in-distribution samples and reject OOD samples. 

Closely related to one-class classification, is estimating the support of the data distribution. In fact, \citet{platt1999estimating} formulate the problem of estimating the support as finding a function which is positive in high density areas and negative everywhere else. Thus connecting density estimation with one-class classification. In the next section, we will look at methods which rely on density estimation. 

\subsection{Density based models}

In general, density based model aim to learn an estimator for the true density of the training distribution \pdf[train]{x}. 
\citep{breunig2000lof, zhang2009new} use local density estimation to compute outlier scores. \citep{qin2016kelos, he2004novelty, bishop1994novelty} use variants of kernel density estimation to produce outlier scores. All those approaches are based on local non-paramteric density estimation schemes. The general intuition here is that regions of input space which have a high density of training samples will have low outlier score, and as the density of points decreases the outlier score gets higher. 

Another approach to density estimation is learn a parametric model of the training data distribution, and use it to estimate the probability of an input. 

% \citet{lee2018simple} learn a distribution over the latent activations of trained deep models on normal inputs, then reject inputs that produce low probability activations. 

\subsection{Generative modelling}

For complex high-dimensional distributions, modern approaches typically rely on deep learning machinery, particularly density estimation with deep networks. Deep learning approaches to density estimation rely on generative modelling. The two most common approaches are Autoencoding~\cite{kingma2013auto} and Generative Adversarial Training~\cite{goodfellow2014generative}. Autoencoding means training a model to somehow compress the data, then reconstruct it while minimizing the NLL of the reconstruction and the length of the compressed data.
Generative adversarial training seeks to train a data generator that mimics the data distribution. Variants of both approaches have been used in the context of anomaly/OOD detection. Moreover, ideas from GANs have been ported to autoencoders(AEs), producing models like the Adversarial Autoencoders(AAes)\citep{makhzani2015adversarial}, and VAE/GAN\citep{larsen2015autoencoding}, which allow for flexible priors in the latent and output space respectively. Presumably allowing for better compression of the data.

% Normalizing flows \cite{rezende2015variational} are another recent deep distribution modeling technique, which uses invertible models.
% As generative models capture the data distribution, they can be used to flag low probability inputs. Autoencoders are readily usable for OOD detection, because their reconstruction error can be directly used as a proxy for the probability of an input. Intuitively, a model trained to reconstruct inputs from a certain distribution, should have a higher reconstruction error for inputs far away from that distribution. 

\citet{xu2018unsupervised, an2015variational, borghesi2019anomaly} all use the reconstruction errors of autoencoders to detect anomalous/OOD inputs. 
% \citet{zhou2017anomaly} perform PCA over the data, splitting it into a low-dimensional 'signal' component and the rest is considered noise. The Autoencoder is then trained to 
\citet{chen2017outlier} use dropout to leverage an ensemble of autoencoders for anomaly detection, the reconstruction error of the ensemble is used as an anomaly score.
\citet{baur2018deep} use the reconstruction error of a VAE/GAN\citep{larsen2015autoencoding}, to detect anomalies in brain MR images. 

One issue with the aforementioned works using the reconstruction error alone is that there is no taking into account how much compression was actually achieved for a particular input.
With VAEs(and variants), it is possible to compute the cost of the latent code for each input, which represents its likelihood in latent space. 

\citet{leveau2017adversarial} use the likelihood of an AAE to compute anomaly scores. The prior is chosen to a either a standard Gaussian, or a mixture of Gaussians. Additionally, they propose a scheme where the prior contains a component modelling anomalies. 

\cite{vasilev2018q} propose using the full VAE objective as an anomaly score. \citet{zong2018deep} propose deriving a likelihood using both the latent representation and the reconstruction error of an AE. During training a Gaussian Mixture Model is fitted to maximize the likelihood of the reconstructing residuals concatenated with latent representations for the inputs. 
\citet{pidhorskyi2018generative} use the likelihood of an AAE to produce anomaly scores. However, they break down points into two components, a projection onto the learned the data manifold, and a \emph{noise} component orthogonal to the manifold. The likelihood of a point is then the joint probability of the components. 

Adversarial discriminators have also been employed for anomaly/OOD detection. In general, GANs can be viewed as a one-class classifier(the discriminator), along with a generator used to provide good negative samples during training. Making them also amenable for anomaly/OOD tasks.
In \citep{sabokrou2018adversarially}, the authors use a denoising AE, coupled with a discriminator network. The idea is that the reconstruction loss is replaced by the discriminator. The reconstructed samples are fed into a discriminator, which then outputs an anomaly score. The whole model is trained as a GAN, and an additional reconstruction loss over the generator. In \citep{perera2019ocgan}, a denoising AE is used, and coupled with two discriminators, a latent space discriminator and an output space discriminator. The latent discriminator is used to enforce that the prior and posterior match in latent space. The output discriminator is a typical GAN discriminator, used to differentiate between generated and real samples.  

\subsubsection{Time series}

In the context of time series, prediction based approaches have been used for anomaly detection. \citep{hundman2018detecting, taylor2016anomaly, malhotra2015long, marchi2015non} all use the prediction error of a vanilla LSTM to detect anomalies. 
In those approaches, the models are trained to predict a future window, then the error in the prediction is used as an anomaly score. 

Those approaches do no regularize the latent state, or take it into account when computing anomaly scores. \citet{guo2018multidimensional} use a GRU based autoencoder, and regularize the latent representation using a Gaussian mixture prior. Still, only the reconstruction error is taken into account when computing anomaly scores.


\subsection{Contrastive approaches}
Those are approaches based on obtaining negative samples. The goal of the negative samples is to simulate OOD data, or delineate the training distribution by lying around the boundary between the training data and everything else. 
If such data is available, the task of OOD detection is greatly simplified, and can be reduced to a normal two-class classification. Alternatively, the model can be trained to have a high uncertainty score on the negative samples. 

There are three sources of negative samples used
\begin{itemize}
    \item An auxiliary data source
    \item A generative model
    \item A noise source
\end{itemize}{}

\textbf{Auxiliary data:} Here the model assumes the availability of some dataset which simulates OOD data. For example, \citet{hendrycks2018deep} apply their outlier exposure model to vision tasks, and the 80 million tiny images\citep{torralba200880} dataset as a stand in for OOD data.

\textbf{A generative model:} \cite{vernekar} use a VAE to model the data distribution. The trained VAE is then used to generate OOD samples which are close to the data distribution. The authors simulate two types of OOD data, one by adding perturbations to the inputs, the second by sampling from low probability regions of the VAE's posterior.

We already discussed GANs, which can be interpreted as a contrastive approach to one-class classification in the previous subsection. Here the discriminator is viewed as a classifier between in and out of distribution, the generator is used to generate good negative samples. Normally, the GAN generator is trained to mimic the training distribution, and if it becomes too good at that, the negative and positive samples start overlapping, and it becomes more challenging to learn a good discriminative boundaries. In the context of semi-supervised learning, \citep{dai2017good} introduced the idea of a bad GAN; a GAN where the generator is imperfect in the sense that its samples lie on the boundary of the data distribution, rather than in the high density regions.

Subsequently, the idea of using a GAN generator which outputs samples on the boundaries of the high density regions, rather than \textbf{in} the high density regions has been used for OOD detection. 
\cite{lee2017training} train GAN generator to produce samples where the classifier is uncertain. The standard cross-entropy loss of the classifier makes it confident in high density areas. This in turn means the generator avoids such regions and goes to the tails of the data distribution. The classifier is then trained to be uncertain over the generated samples. 

\textbf{Random noise sources:} Using noise can be much simpler than the generative approaches discussed above. There are various approaches in the literature which use noise to  simulate OOD data. 

\cite{jain2019maximizing} use samples from a uniform distribution over the input space, in an attempt to cover all possible inputs. Then an ensemble is trained to have high diversity on those noise samples, thus increasing uncertainty. \citet{Hafner2018NoiseCP} introduce noise contrastive priors(NCP) as a technique for constructing a prior which favors uncertain functions outside the data distribution. In their applications, they add Gaussian noise to the training data in order to simulate OOD inputs, then train a model to predict high uncertainty on those OOD inputs.

The issue with those generic noise models is that we are unlikely to get the OOD samples we want; samples which cover the surface around high density regions well, but do not overlap with them. For high dimensional input spaces, a large number of samples is required to cover all the space. Adding Gaussian noise to inputs is problematic, since, on one hand, too little noise would make our noisy samples overlap too much with our training data, On the other hand, \cite{csiszarik2019negative} argue that with enough noise, the samples become too easy to distinguish from real data. Intuitively, this is saying that while the corrupted samples are not too far in terms of euclidean distance, they are \emph{semantically} too far from the data distribution to provide the required regularization. To mitigate those problems, \cite{hein2019relu} use adversarial noise added to the inputs. The noise is sculpted to maximize the prediction confidence over the corrupted samples. This scheme is proposed to effectively cover the boundary between the training data and the rest of input space. Thus we can see that at least in its motivation, this method also seeks to find a boundary of the training data distribution. 


The general theme of the approaches discussed here is that dealing with OOD inputs requires explicit or implicit modelling of the boundaries of the training data distribution, or an estimator for the training distribution density. Similarity based approaches use a parametric model of the boundary, density based approaches measure the density around a point, generative models model the training distribution, and contrastive approaches use noise source to simulate the boundary between training distribution and the rest of the input space.   




\end{document}